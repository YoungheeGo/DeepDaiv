# -*- coding: utf-8 -*-
"""CNN for Text Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Qa44wK1NoIGsxH3fv8qrldMcqGnSpTTS
"""

import numpy as np
import pandas as pd
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

from keras.layers import Input, Embedding, Activation, Flatten, Dense
from keras.layers import Conv1D, MaxPooling1D, Dropout
from keras.models import Model

from google.colab import drive
drive.mount('/content/drive')

cd drive/MyDrive/Deep\ Daiv

"""# Load Data
- preprocessing
  1. data: label, text 열로 변경
  2. 텍스트는 소문자로 변경
  3. Tokenizer : char-level로 토큰화
  4. Construct Vocab
  5. String to index
  6. Padding
  7. Get Label
"""

train_df=pd.read_csv('AG_news/train.csv')
test_df=pd.read_csv('AG_news/test.csv')

train_df.rename(columns={'Class Index':0,'Title':1,'Description':2},inplace=True)
test_df.rename(columns={'Class Index':0,'Title':1,'Description':2},inplace=True)

train_df.head()

test_df.head()

"""#### *1. dataframe 열 변경*"""

# concatenate column 1 and column 2 as one text
for df in [train_df, test_df]:
  df[1] = df[1] + df[2]
  df = df.drop([2], axis=1)

"""#### *2. 텍스트는 소문자로 변경*"""

# convert string to lower case 
train_texts = train_df[1].values 
train_texts = [s.lower() for s in train_texts]

test_texts = test_df[1].values 
test_texts = [s.lower() for s in test_texts]

"""#### *3. Tokenizer*"""

# Initialization
tk=Tokenizer(num_words=None, char_level=True,oov_token='UNK')

# Fitting
tk.fit_on_texts(train_texts)

"""#### *4. Construct Vocab*"""

## Vocab ##
# construct a new vocabulary
alphabet = "abcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\"/\\|_@#$%^&*~`+-=<>()[]{}"
char_dict = {}
for i, char in enumerate(alphabet):
    char_dict[char] = i + 1

# Use char_dict to replace the tk.word_index
tk.word_index = char_dict.copy()
# Add 'UNK' to the vocabulary
tk.word_index[tk.oov_token] = max(char_dict.values()) + 1
# oov_token: Out Of Vocabulary (oov) -> 모르는 단어로 인해 문제를 푸는 것이 까다로워지는 상황 처리함

"""#### *5. string to index*"""

# Convert string to index
train_sequences = tk.texts_to_sequences(train_texts)
test_texts = tk.texts_to_sequences(test_texts)

"""#### *6. Padding*"""

train_data = pad_sequences(train_sequences, maxlen=1014, padding='post')
test_data = pad_sequences(test_texts, maxlen=1014, padding='post')

# Convert to numpy array
train_data = np.array(train_data, dtype='float32')
test_data = np.array(test_data, dtype='float32')

"""#### *7. Get Label*"""

train_classes = train_df[0].values
train_class_list = [x - 1 for x in train_classes]

test_classes = test_df[0].values
test_class_list = [x - 1 for x in test_classes]

from tensorflow.keras.utils import to_categorical

train_classes = to_categorical(train_class_list)
test_classes = to_categorical(test_class_list)

"""# Char CNN

1. Parameter
2. Embedding Layer
3. Model Construction
4. Shuffle
5. Training
6. Visualization

#### *1. Parameter*
"""

input_size = 1014
vocab_size = len(tk.word_index)
embedding_size = 69
conv_layers = [[256, 7, 3],
               [256, 7, 3],
               [256, 3, -1],
               [256, 3, -1],
               [256, 3, -1],
               [256, 3, 3]]

fully_connected_layers = [1024, 1024]
num_of_classes = 4
dropout_p = 0.5
optimizer = 'adam'
loss = 'categorical_crossentropy'

"""#### *2. Embedding Layer*"""

# Embedding weights
embedding_weights = []  # (70, 69)
embedding_weights.append(np.zeros(vocab_size))  # (0, 69)

for char, i in tk.word_index.items():  # from index 1 to 69
    onehot = np.zeros(vocab_size)
    onehot[i - 1] = 1
    embedding_weights.append(onehot)

embedding_weights = np.array(embedding_weights)
print('Load')

# Embedding layer Initialization
embedding_layer = Embedding(vocab_size + 1,
                            embedding_size,
                            input_length=input_size,
                            weights=[embedding_weights])



"""#### *3. Model Construction*"""

# Input
inputs = Input(shape=(input_size,), name='input', dtype='int64')  # shape=(?, 1014)
# Embedding
x = embedding_layer(inputs)
# Conv
for filter_num, filter_size, pooling_size in conv_layers:
    x = Conv1D(filter_num, filter_size)(x)
    x = Activation('relu')(x)
    if pooling_size != -1:
        x = MaxPooling1D(pool_size=pooling_size)(x)  # Final shape=(None, 34, 256)
x = Flatten()(x)  # (None, 8704)
# Fully connected layers
for dense_size in fully_connected_layers:
    x = Dense(dense_size, activation='relu')(x)  # dense_size == 1024
    x = Dropout(dropout_p)(x)
# Output Layer
predictions = Dense(num_of_classes, activation='softmax')(x)
# Build model
model = Model(inputs=inputs, outputs=predictions)
model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])  # Adam, categorical_crossentropy
model.summary()

"""#### *4. Shuffle*"""

indices = np.arange(train_data.shape[0])
np.random.shuffle(indices)

x_train = train_data[indices]
y_train = train_classes[indices]

x_test = test_data
y_test = test_classes

"""#### *5. Training*"""

learning_history=model.fit(x_train, y_train,
          validation_data=(x_test, y_test),
          batch_size=128,
          epochs=10,
          verbose=2)

"""#### 6. Visualization"""

# 학습결과 시각화

import matplotlib.pyplot as plt

hist = pd.DataFrame(learning_history.history)
hist['epoch'] = learning_history.epoch
hist.tail()


plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.plot(hist['epoch'], hist['accuracy'], label = 'Train accuracy')
plt.plot(hist['epoch'], hist['val_accuracy'], label = 'Val accuracy')
plt.legend()
plt.show()











